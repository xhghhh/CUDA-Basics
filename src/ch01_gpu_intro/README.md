1. GPU：Graphics Processing Unit（图形处理单元），擅长大规模并行计算；CPU：Central Processing Unit（中央处理单元），擅长逻辑控制与通用计算。

2. CPU 与 GPU 架构的区别：

   * **核心数量**：CPU 通常拥有少量高性能核心（如 4～16 个），用于执行复杂的顺序任务；GPU 拥有大量计算核心（数千个），专为并行处理大量简单任务设计。
   * **控制逻辑 vs. 运算单元**：CPU 的芯片面积中很大一部分用于复杂的控制逻辑和缓存；GPU 则将更多晶体管用于算术逻辑单元（ALU），以提升计算吞吐量。
   * **内存层次结构**：CPU 具有多级缓存（L1/L2/L3）和复杂的预测、乱序执行机制；GPU 使用共享内存、寄存器和全局显存，优化大规模数据并行访问。
   * **任务特征**：CPU 适合低延迟、逻辑复杂的任务（如系统调度、串行计算）；GPU 适合高吞吐、计算密集型任务（如矩阵运算、图像渲染、深度学习）。
   * **执行模型**：CPU 基于 MIMD（Multiple Instruction, Multiple Data）；GPU 多采用 SIMT（Single Instruction, Multiple Threads），同一指令并行执行多个线程的数据流。

3. GPU 计算的执行模型为 CPU（host） + GPU（device）：CPU 负责程序控制、任务分配与内存管理；GPU 负责大规模并行计算。两者通过 PCIe 总线通信与数据传输，受限于带宽和延迟。

4. FLOPS（Floating Point Operations Per Second）：衡量计算性能的指标，表示每秒可执行的浮点运算次数；TFLOPS（Tera FLOPS）为每秒万亿次浮点运算，常用于评估 GPU 的理论峰值计算能力。

5. 计算能力（Compute Capability）以 X.Y 表示，用于区分 GPU 架构与硬件特性，例如支持的线程数量、共享内存大小、硬件指令集等。不同版本对应不同 CUDA 特性。

6. 显存容量（GB）：GPU 的存储空间，用于保存数据、权重、纹理、临时变量等。显存越大，可并行处理的数据规模越大。

7. 显存带宽（GB/s）：表示显存与计算核心之间的数据传输速率。带宽越高，数据吞吐能力越强，对内存密集型计算影响显著。

8. 使用 `nvidia-smi` 命令可查看 GPU 信息，包括型号、显存使用率、功耗、温度、驱动版本、计算进程等，是 CUDA 环境调试与资源监控的重要工具。
